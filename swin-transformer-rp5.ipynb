{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-20T05:23:40.434674Z","iopub.execute_input":"2025-02-20T05:23:40.434986Z","iopub.status.idle":"2025-02-20T05:23:43.381902Z","shell.execute_reply.started":"2025-02-20T05:23:40.434949Z","shell.execute_reply":"2025-02-20T05:23:43.380826Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom timm.models.swin_transformer import SwinTransformer\n\n# Shared Spatial Attention Module\ndef shared_spatial_attention(x):\n    B, C, H, W = x.shape\n    q = F.normalize(x, p=2, dim=1)\n    k = F.normalize(x, p=2, dim=1)\n    v = x.view(B, C, -1)\n    attn = torch.matmul(q.view(B, C, -1), k.view(B, C, -1).transpose(-2, -1))\n    return torch.matmul(attn, v).view(B, C, H, W)\n\n# Shared Channel Attention Module\ndef shared_channel_attention(x):\n    B, C, H, W = x.shape\n    q = F.adaptive_avg_pool2d(x, 1).view(B, C, 1, 1)\n    k = F.adaptive_max_pool2d(x, 1).view(B, C, 1, 1)\n    attn = torch.sigmoid(q + k)\n    return x * attn\n\n# Aggregation Feature Module\nclass AggregationFeature(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        return x\n\n# Swin Transformer Encoder-Decoder Architecture for Akshu Region Dataset\nclass SwinSegmentation(nn.Module):\n    def __init__(self, img_size=512, num_classes=9, in_channels=4):  # Adjusted for multispectral data\n        super().__init__()\n        self.encoder = SwinTransformer(img_size=img_size, in_chans=in_channels, num_classes=0, pretrained=False)\n        self.conv1x1 = nn.Conv2d(768, 256, kernel_size=1)  # Adjust channels from Swin Transformer output\n        self.decoder = nn.ModuleList([\n            AggregationFeature(256, 128),\n            AggregationFeature(128, 64),\n            AggregationFeature(64, 32)\n        ])\n        self.final_seg = nn.Conv2d(32, num_classes, kernel_size=1)\n\n    def forward(self, x):\n        enc_features = self.encoder.forward_features(x)  # Output shape: (B, H/32, W/32, C)\n        enc_features = enc_features.permute(0, 3, 1, 2)  # Reshape to (B, C, H, W)\n        enc_features = self.conv1x1(enc_features)  # Reduce channels\n        \n        for af in self.decoder:\n            enc_features = af(enc_features)\n        \n        combined = shared_spatial_attention(enc_features) + shared_channel_attention(enc_features)\n        segmentation_output = self.final_seg(combined)\n        return segmentation_output\n\n# Example Usage\nmodel = SwinSegmentation(num_classes=9, in_channels=4)  # Adjusted for Akshu region dataset\ninput_tensor = torch.randn(1, 4, 512, 512)  # 4-channel multispectral input\noutput = model(input_tensor)\nprint(\"Output Shape:\", output.shape)  # Expected: (1, num_classes, 512, 512)\nprint(output)\nprint(type(output))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T06:00:10.578379Z","iopub.execute_input":"2025-02-20T06:00:10.578714Z","iopub.status.idle":"2025-02-20T06:00:11.180084Z","shell.execute_reply.started":"2025-02-20T06:00:10.578687Z","shell.execute_reply":"2025-02-20T06:00:11.178565Z"}},"outputs":[{"name":"stdout","text":"Output Shape: torch.Size([1, 9, 16, 16])\ntensor([[[[-2.1036, -2.1275, -2.1643,  ..., -2.1359, -2.1392, -2.0001],\n          [-2.0955, -2.1237, -2.1378,  ..., -2.1453, -2.1697, -2.0552],\n          [-2.1051, -2.1198, -2.1335,  ..., -2.1584, -2.1378, -2.0366],\n          ...,\n          [-2.1277, -2.1140, -2.1398,  ..., -2.1097, -2.1495, -2.0509],\n          [-2.1127, -2.1120, -2.1354,  ..., -2.1099, -2.1570, -2.0256],\n          [-2.0397, -2.0293, -2.0287,  ..., -2.0031, -1.9899, -1.9635]],\n\n         [[ 3.6570,  3.7035,  3.7689,  ...,  3.7192,  3.7249,  3.4715],\n          [ 3.6399,  3.6970,  3.7214,  ...,  3.7339,  3.7779,  3.5707],\n          [ 3.6585,  3.6890,  3.7125,  ...,  3.7575,  3.7208,  3.5384],\n          ...,\n          [ 3.6988,  3.6798,  3.7236,  ...,  3.6703,  3.7418,  3.5634],\n          [ 3.6719,  3.6741,  3.7162,  ...,  3.6717,  3.7562,  3.5184],\n          [ 3.5344,  3.5200,  3.5184,  ...,  3.4723,  3.4493,  3.4021]],\n\n         [[ 9.8113,  9.9230, 10.0921,  ...,  9.9611,  9.9764,  9.3246],\n          [ 9.7731,  9.9056,  9.9672,  ..., 10.0002, 10.1132,  9.5791],\n          [ 9.8190,  9.8814,  9.9445,  ..., 10.0618,  9.9664,  9.4961],\n          ...,\n          [ 9.9234,  9.8607,  9.9722,  ...,  9.8351, 10.0184,  9.5631],\n          [ 9.8556,  9.8448,  9.9567,  ...,  9.8384, 10.0568,  9.4462],\n          [ 9.5119,  9.4654,  9.4601,  ...,  9.3438,  9.2825,  9.1548]],\n\n         ...,\n\n         [[-8.9439, -9.0472, -9.2016,  ..., -9.0818, -9.0959, -8.5002],\n          [-8.9093, -9.0299, -9.0866,  ..., -9.1163, -9.2200, -8.7328],\n          [-8.9508, -9.0078, -9.0664,  ..., -9.1720, -9.0851, -8.6566],\n          ...,\n          [-9.0460, -8.9889, -9.0914,  ..., -8.9650, -9.1330, -8.7175],\n          [-8.9833, -8.9757, -9.0754,  ..., -8.9681, -9.1686, -8.6113],\n          [-8.6683, -8.6247, -8.6205,  ..., -8.5137, -8.4574, -8.3446]],\n\n         [[ 2.5634,  2.5928,  2.6382,  ...,  2.6023,  2.6070,  2.4326],\n          [ 2.5516,  2.5877,  2.6035,  ...,  2.6128,  2.6427,  2.5016],\n          [ 2.5643,  2.5816,  2.5984,  ...,  2.6297,  2.6032,  2.4791],\n          ...,\n          [ 2.5908,  2.5754,  2.6049,  ...,  2.5678,  2.6176,  2.4974],\n          [ 2.5730,  2.5705,  2.5999,  ...,  2.5682,  2.6271,  2.4660],\n          [ 2.4793,  2.4671,  2.4646,  ...,  2.4340,  2.4169,  2.3860]],\n\n         [[-4.5511, -4.6050, -4.6855,  ..., -4.6233, -4.6310, -4.3219],\n          [-4.5327, -4.5965, -4.6269,  ..., -4.6431, -4.6976, -4.4433],\n          [-4.5547, -4.5867, -4.6172,  ..., -4.6726, -4.6276, -4.4035],\n          ...,\n          [-4.6032, -4.5757, -4.6305,  ..., -4.5648, -4.6530, -4.4345],\n          [-4.5724, -4.5697, -4.6222,  ..., -4.5658, -4.6702, -4.3790],\n          [-4.4066, -4.3863, -4.3843,  ..., -4.3285, -4.2995, -4.2400]]]],\n       grad_fn=<ConvolutionBackward0>)\n<class 'torch.Tensor'>\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install rasterio\n!pip install tifffile\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T06:28:27.899709Z","iopub.execute_input":"2025-02-20T06:28:27.900088Z","iopub.status.idle":"2025-02-20T06:28:35.564911Z","shell.execute_reply.started":"2025-02-20T06:28:27.900061Z","shell.execute_reply":"2025-02-20T06:28:35.563580Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: rasterio in /usr/local/lib/python3.10/site-packages (1.4.3)\nRequirement already satisfied: click>=4.0 in /usr/local/lib/python3.10/site-packages (from rasterio) (8.1.8)\nRequirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/site-packages (from rasterio) (2.0.2)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/site-packages (from rasterio) (3.2.1)\nRequirement already satisfied: affine in /usr/local/lib/python3.10/site-packages (from rasterio) (2.4.0)\nRequirement already satisfied: click-plugins in /usr/local/lib/python3.10/site-packages (from rasterio) (1.1.1)\nRequirement already satisfied: attrs in /usr/local/lib/python3.10/site-packages (from rasterio) (25.1.0)\nRequirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/site-packages (from rasterio) (0.7.2)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/site-packages (from rasterio) (2025.1.31)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nCollecting tifffile\n  Downloading tifffile-2025.2.18-py3-none-any.whl (226 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.4/226.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from tifffile) (2.0.2)\nInstalling collected packages: tifffile\nSuccessfully installed tifffile-2025.2.18\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom timm.models.swin_transformer import SwinTransformer\nfrom torchvision import transforms\nimport cv2\nimport numpy as np\nimport tifffile as tiff\n\n# Load and preprocess satellite image\ndef load_satellite_image(image_path):\n    image = tiff.imread(image_path)  # Load TIFF image\n    if image.shape[0] > 3:  # If more than 3 channels, take only the first 3 (RGB)\n        image = image[:3, :, :]\n    image = np.transpose(image, (1, 2, 0))  # Convert to (H, W, C) format\n    image = cv2.resize(image, (512, 512))  # Resize\n    image = image.astype(np.float32) / 255.0  # Normalize\n    image = torch.tensor(image).permute(2, 0, 1).unsqueeze(0)  # Convert to (B, C, H, W)\n    \n    return image\n\n# Shared Spatial Attention Module\ndef shared_spatial_attention(x):\n    B, C, H, W = x.shape\n    q = F.normalize(x, p=2, dim=1)\n    k = F.normalize(x, p=2, dim=1)\n    v = x.view(B, C, -1)\n    attn = torch.matmul(q.view(B, C, -1), k.view(B, C, -1).transpose(-2, -1))\n    return torch.matmul(attn, v).view(B, C, H, W)\n\n# Shared Channel Attention Module\ndef shared_channel_attention(x):\n    B, C, H, W = x.shape\n    q = F.adaptive_avg_pool2d(x, 1).view(B, C, 1, 1)\n    k = F.adaptive_max_pool2d(x, 1).view(B, C, 1, 1)\n    attn = torch.sigmoid(q + k)\n    return x * attn\n\n# Aggregation Feature Module\nclass AggregationFeature(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        return x\n\n# Swin Transformer Encoder-Decoder Architecture for Akshu Region Dataset\nclass SwinSegmentation(nn.Module):\n    def __init__(self, img_size=512, num_classes=5, in_channels=3):  # Adjusted for RGB satellite data\n        super().__init__()\n        self.encoder = SwinTransformer(img_size=img_size, in_chans=in_channels, num_classes=0, pretrained=False)\n        self.conv1x1 = nn.Conv2d(768, 256, kernel_size=1)  # Adjust channels from Swin Transformer output\n        self.decoder = nn.ModuleList([\n            AggregationFeature(256, 128),\n            AggregationFeature(128, 64),\n            AggregationFeature(64, 32)\n        ])\n        self.final_seg = nn.Conv2d(32, num_classes, kernel_size=1)\n\n    def forward(self, x):\n        enc_features = self.encoder.forward_features(x)  # Output shape: (B, H/32, W/32, C)\n        enc_features = enc_features.permute(0, 3, 1, 2)  # Reshape to (B, C, H, W)\n        enc_features = self.conv1x1(enc_features)  # Reduce channels\n        \n        for af in self.decoder:\n            enc_features = af(enc_features)\n        \n        combined = shared_spatial_attention(enc_features) + shared_channel_attention(enc_features)\n        segmentation_output = self.final_seg(combined)\n        return segmentation_output\n\n# Load an example satellite image and perform segmentation\nimage_path = \"/kaggle/input/akshu-dataset/Aksu/Test/Image/true_color_image_02_07.tif\"  # Replace with actual image path\ninput_image = load_satellite_image(image_path)\n\n# Initialize model and perform inference\nmodel = SwinSegmentation(num_classes=5, in_channels=3)  # Adjusted for RGB satellite image\noutput = model(input_image)\nprint(\"Output Shape:\", output.shape)  # Expected: (1, num_classes, 512, 512)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T06:28:43.726567Z","iopub.execute_input":"2025-02-20T06:28:43.726914Z","iopub.status.idle":"2025-02-20T06:28:44.474219Z","shell.execute_reply.started":"2025-02-20T06:28:43.726884Z","shell.execute_reply":"2025-02-20T06:28:44.472718Z"}},"outputs":[{"name":"stdout","text":"Output Shape: torch.Size([1, 5, 16, 16])\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}